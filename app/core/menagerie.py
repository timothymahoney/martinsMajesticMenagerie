import jsonimport requestsfrom pathlib import Pathimport osimport sysimport loggingimport reimport pandas as pdclass app():    def __init__(self, target="https://anapioficeandfire.com", logmode="all", pagesize=50, targetObject="characters"):        self.target = target        self.cache = os.path.join(str(Path.home()), ".appcache")  #todo allow for a path override here        #self.checks = []        # initialize the cache        self.home = self._makeCache()        self.rawcache = os.path.join('')        self.collectionPageSize = pagesize        self.targetObject = targetObject        if logmode == "all":            self.log = self.__createLogger(logfile=os.path.join(self.cache, "log"), stdout=True)        elif logmode == "stdout":            self.log = self.__createLogger(stdout=True)        elif logmode == "file":            self.log = self.__createLogger(logfile=os.path.join(self.cache, "log"))    # @property    # def checks(self):    #     return self.checks    #    # @checks.setter    # def checks(self, value):    #     self.checks.append(value)    #    # def __handle_checks(self):    #     if len(self.checks(0)):    #         return    #     else:    #         message = ""    #         # process each check statement    #         raise SyntaxError([message.join(" | ") for i in self.checks])    def _makeCache(self):        resp = {}  # dict of paths to our vital bits        _rawcache = os.path.join(self.cache, "raw")        _cleancache = os.path.join(self.cache, "clean")        try:            if os.path.exists(self.cache):                resp['cache'] = self.cache            else:                os.mkdir(self.cache)                resp['cache'] = self.cache            if os.path.exists(_rawcache):                resp['rawcache'] = _rawcache            else:                os.mkdir(_rawcache)                resp['rawcache'] = _rawcache            if os.path.exists(_cleancache):                resp['cleancache'] = _cleancache            else:                os.mkdir(_cleancache)                resp['cleancache'] = _cleancache        except IOError:            sys.exit("illegal operation on {}".format(self.cache))        return resp    def __createLogger(self, logfile=None, stdout=None):        # stdout logger        root = logging.getLogger()        root.setLevel(logging.DEBUG)        handlers = []        if logfile is not None:            fh = logging.FileHandler(filename=logfile)            handlers.append(fh)        if stdout is not None:            stdouth = logging.StreamHandler(sys.stdout)            handlers.append(stdouth)        if len(handlers) > 0:            logging.basicConfig(level=logging.DEBUG, handlers=handlers)            applog = logging.getLogger('martinsMajesticMenagerie')            return applog        else:            raise SyntaxError("ERROR! Cannot initialize logger. Neither stdout or file logging specified")    def collectTargetTypes(self):        """        collect the api endpoints for use        :return: a dictionary of the api endpoints        :rtype: dict        """        try:            req = requests.get(url="{}/api".format(self.target))        # way too many things could go wrong here, so for now we'll keep this broad        except Exception:            msg = "failed to collect api endpoints!"            return {"message": msg}        return req.json()    def collectTarget(self):        """        Get the bits, write them to the cache. To keep this simple (and raw json) we're going to dump each page of        results to its own file in self.cache/raw/        :return: a status message dictionary        :rtype: dict        """        page = 1        # get the first to start us off processing headers and making raw cachefiles        try:            req = requests.get(url=self.__objectTarget(page=page))        # way too many things could go wrong here, so for now we'll keep this broad        # we're going to return a message dictionary here too, since there's no way we'd be able to continue collection        except Exception:            msg = "failed to collect from target: {}!".format(self.__objectTarget(page=page))            return {"message": msg}        # process results of first pull        self.__writeRaw(pagenumber=page, results=req.json())        # indent the page count so we don't collect this one again        page += 1        # lazy method: derive the number of pages to get a range        all_targets = range(page, self.__resolvepages(linkheader=req.headers['link']))        logging.debug("{} pages of content identifed. Preparing to collect the remaining {}".format(            len(all_targets), all_targets[-1]))        # alt method: headermatch        # while self.__headermatch(linkheader=req.headers['link'], target=req.url):        #     req = requests.get(url=self.__formtarget(page=page))        #     self.__writeRaw(pagenumber=page, results=req.json())        #     page += 1        # walk the range to collect all our goodies        for i in all_targets:            req = requests.get(url=self.__objectTarget(page=page))            self.__writeRaw(pagenumber=page, results=req.json())            page += 1        return {"collected_pages": page}    def __objectTarget(self, page):        """        create a url target        :param page: the pagenumber to collect        :return: a target url        :rtype: string        """        return "{}/api/{}?page={}&pageSize=50".format(self.target, self.targetObject, page, self.collectionPageSize)    def __resolvepages(self, linkheader):        """        given the link section of the headers, evaluate the crazy string for the rel="last" object, then separate it and        its url, then regex the url for the pages section and return the page value        :param linkheader: the 'link' header value        :return: the number of the last page of results defined in the linkheader        :type: int        """        res = linkheader.split(",")        # get the next value        last = [i for i in res if 'rel="last"' in i][0]        pages = "(?:page=)(\d*)"        return int(re.findall(pattern=pages, string=[i for i in res if 'rel="last"' in i][0])[0])    def __headermatch(self, linkheader, target):        """        given the link section of the headers, evaluate the crazy string for the rel="last" object, separate it and then        its url, and then if that url matches the target parameter return True.        :param linkheader: the 'link' header value        :param target: a url to compare against the rel="last" section of the linkheader        :return: Bool        :rtype: bool        """        res = linkheader.split(";")        # get the next value        last = [i for i in res if 'rel="last"' in i][0]        # regex happily adapted from regexr.com/36fcc. Thank you Amar Palsapure        url = "(https): // ([\w+?\.\w+]) + ([a - zA - Z0 - 9\~\!\ @ \  # \$\%\^\&\*\(\)_\-\=\+\\\/\?\.\:\;\'\,]*)?"        if re.search(pattern=url, string=[i for i in res if 'rel="last"' in i][0]).lower() == target:            return True        else:            return False    def __writeRaw(self, pagenumber, results):        """        write json to file in the cache as the self.targetObject-pagenumber.        No returns as I'm getting tired and if that fails we're going to simply exit.        :param pagenumber: the pagenumber of results        :param results: the json results to store        """        _cachefile = os.path.join(self.home['rawcache'], "{}-{}".format(self.targetObject, pagenumber))        try:            with open(_cachefile, 'w') as f:                json.dump(results, f)            logging.debug("writing out new cache file at: {}".format(_cachefile))        except IOError:            msg = "error caching results from page: {} at path: {}".format(pagenumber, _cachefile)            logging.error(msg)            # hard stop, no cache is inefficient and out of scope at this point            sys.exit(msg)    def __processdates(self, record):        """        generator function that converts the born and died fields into something meaningful based on the common era: AC        :return:        """        # grab the date digits from an In ... statement        dregex = "([\d]{1,3})(?!AC|BC)"        # if we don't have a born date and/or died date we cant normalize dates, so move on        _resp = record        _resp['died_ac'] = None        _resp['born_ac'] = None        try:            if len(record['born']) > 6:                if "AC" in record['born']:                    date = re.search(pattern=dregex, string=record['born'])                    if date is not None:                        _resp['born_ac'] = date.group(0)                if "BC" in record['born']:                    date = re.search(pattern=dregex, string=record['born'])                    if date is not None:                        _resp['died_ac'] = date.group(0)                else:                    logging.info("unknown time object in born field of record:{}".format(_resp))            if len(record['died']) > 6:                if "AC" in record['died']:                    date = re.search(pattern=dregex, string=record['died'])                    if date is not None:                        _resp['died_ac'] = date.group(0)                elif "BC" in record['died']:                    date = re.search(pattern=dregex, string=record['died'])                    if date is not None:                        _resp['died_ac'] = date.group(0)                else:                    logging.info("unknown time object in died field of record:{}".format(record))        except ValueError:            logging.info("unknown time object in record:{}".format(record))        return _resp    def normalize(self, sample=0):        """        read in all the raw cached json into a dataframe        :return:        """        _load = []        # iterate through the raw cache        cachefiles = os.listdir(self.home['rawcache'])        if len(cachefiles) > 0:            if sample == 0:                loadrange = range(0, len(cachefiles))            else:                loadrange = range(0, sample)            for val in loadrange:                with open(os.path.join(self.home['rawcache'], cachefiles[val]), 'r') as f:                    for obj in json.load(f):                        _load.append(self.__processdates(record=obj))        else:            logging.error("no raw data cached to normalize in {}!!".format(self.home['rawcache']))            sys.exit()        # once we've got our data processed, dump it to a dataframe for analysis        if len(_load) > 0:            # create a dataframe            df = pd.DataFrame(_load)            # write it to the cache            df.to_csv(os.path.join(self.home['cleancache'], "data.csv"))            df.to_pickle(os.path.join(self.home['cleancache'], "data.pkl"))            return df